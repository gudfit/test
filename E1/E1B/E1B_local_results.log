
Generating probes from /root/test/E1/E1B/../../data/largesample.txt...

Calculating size of Gzip-compressed data: /root/test/E1/E1B/../../data/wikipedia.txt...
Gzip Compressed Data Size (/root/test/E1/E1B/../../data/wikipedia.txt): 2.061511993408203 MB


================== Processing Model: bert-base-cased ==================

PHASE 0: Fine-Tuning...
Fine-tuned model for bert-base-cased already exists. Skipping fine-tuning.

PHASE 1: Static Size vs. Functional Utility...
Assessing Factual Recall for fine-tuned bert-base-cased...
{
    "model_path": "/root/test/E1/E1B/models/bert-base-cased-finetuned-local",
    "assessment": "Factual Recall from Local Data",
    "model_size_mb": 414.1589460372925,
    "factual_recall_score": 0.15
}

PHASE 2: Computational Cost & Adaptability...
NOTE: The following tasks use the base 'bert-base-cased' model for fair comparison.

Assessing Computational Cost for bert-base-cased...
{
    "model_name": "bert-base-cased",
    "assessment": "Computational Cost Analysis",
    "details": {
        "gflops_per_inference": 10.884513792,
        "avg_latency_ms_per_inference": 3.43386173248291,
        "sequence_length": 128
    }
}

Assessing Zero-Shot Reasoning for bert-base-cased...
{
    "model_name": "bert-base-cased",
    "assessment": "Zero-Shot NLI Reasoning",
    "task": "multi_nli",
    "accuracy": 0.3
}

Measuring Data Efficiency for bert-base-cased...
{'train_runtime': 5.2106, 'train_samples_per_second': 57.575, 'train_steps_per_second': 7.485, 'train_loss': 0.6552399366329877, 'epoch': 3.0}
{'loss': 0.6357, 'grad_norm': 9.506585121154785, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.0}
{'train_runtime': 7.7391, 'train_samples_per_second': 77.528, 'train_steps_per_second': 9.691, 'train_loss': 0.4962254206339518, 'epoch': 3.0}
{'loss': 0.5912, 'grad_norm': 4.356448173522949, 'learning_rate': 2.850877192982456e-05, 'epoch': 1.32}
{'loss': 0.1735, 'grad_norm': 10.431181907653809, 'learning_rate': 6.578947368421053e-06, 'epoch': 2.63}
{'train_runtime': 10.3807, 'train_samples_per_second': 86.7, 'train_steps_per_second': 10.982, 'train_loss': 0.3461365396516365, 'epoch': 3.0}
{'loss': 0.6034, 'grad_norm': 37.88584518432617, 'learning_rate': 3.366666666666667e-05, 'epoch': 1.0}
{'loss': 0.2127, 'grad_norm': 1.2907212972640991, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.0}
{'loss': 0.0454, 'grad_norm': 0.26082170009613037, 'learning_rate': 3.3333333333333335e-07, 'epoch': 3.0}
{'train_runtime': 13.1551, 'train_samples_per_second': 91.219, 'train_steps_per_second': 11.402, 'train_loss': 0.28717366377512615, 'epoch': 3.0}
{'loss': 0.5926, 'grad_norm': 19.65119743347168, 'learning_rate': 3.7037037037037037e-05, 'epoch': 0.79}
{'loss': 0.3404, 'grad_norm': 60.977962493896484, 'learning_rate': 2.380952380952381e-05, 'epoch': 1.59}
{'loss': 0.2205, 'grad_norm': 0.10865624248981476, 'learning_rate': 1.0582010582010582e-05, 'epoch': 2.38}
{'train_runtime': 15.9595, 'train_samples_per_second': 93.988, 'train_steps_per_second': 11.842, 'train_loss': 0.32290077587914845, 'epoch': 3.0}
{'loss': 0.5588, 'grad_norm': 15.642796516418457, 'learning_rate': 3.9111111111111115e-05, 'epoch': 0.67}
{'loss': 0.3545, 'grad_norm': 0.9046211242675781, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.33}
{'loss': 0.2839, 'grad_norm': 1.7083629369735718, 'learning_rate': 1.688888888888889e-05, 'epoch': 2.0}
{'loss': 0.1143, 'grad_norm': 132.92274475097656, 'learning_rate': 5.777777777777778e-06, 'epoch': 2.67}
{'train_runtime': 17.8513, 'train_samples_per_second': 100.833, 'train_steps_per_second': 12.604, 'train_loss': 0.2987773974736532, 'epoch': 3.0}
{'loss': 0.6382, 'grad_norm': 21.48541831970215, 'learning_rate': 4.071969696969698e-05, 'epoch': 0.57}
{'loss': 0.3731, 'grad_norm': 10.675006866455078, 'learning_rate': 3.125e-05, 'epoch': 1.14}
{'loss': 0.2562, 'grad_norm': 2.253382682800293, 'learning_rate': 2.178030303030303e-05, 'epoch': 1.7}
{'loss': 0.1408, 'grad_norm': 0.07378089427947998, 'learning_rate': 1.2310606060606061e-05, 'epoch': 2.27}
{'loss': 0.0848, 'grad_norm': 0.17560699582099915, 'learning_rate': 2.840909090909091e-06, 'epoch': 2.84}
{'train_runtime': 20.7176, 'train_samples_per_second': 101.363, 'train_steps_per_second': 12.743, 'train_loss': 0.2840185688074791, 'epoch': 3.0}
{'loss': 0.5865, 'grad_norm': 20.760635375976562, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.5}
{'loss': 0.4787, 'grad_norm': 13.152222633361816, 'learning_rate': 3.35e-05, 'epoch': 1.0}
{'loss': 0.305, 'grad_norm': 14.329702377319336, 'learning_rate': 2.5166666666666667e-05, 'epoch': 1.5}
{'loss': 0.2259, 'grad_norm': 10.286711692810059, 'learning_rate': 1.6833333333333334e-05, 'epoch': 2.0}
{'loss': 0.1023, 'grad_norm': 0.18783050775527954, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.5}
{'loss': 0.0824, 'grad_norm': 0.12908144295215607, 'learning_rate': 1.6666666666666668e-07, 'epoch': 3.0}
{'train_runtime': 23.3932, 'train_samples_per_second': 102.594, 'train_steps_per_second': 12.824, 'train_loss': 0.2968128188451131, 'epoch': 3.0}
{'loss': 0.6244, 'grad_norm': 5.1617350578308105, 'learning_rate': 4.2772861356932154e-05, 'epoch': 0.44}
{'loss': 0.4557, 'grad_norm': 30.8944034576416, 'learning_rate': 3.5398230088495574e-05, 'epoch': 0.88}
{'loss': 0.2627, 'grad_norm': 0.8077417016029358, 'learning_rate': 2.8023598820059e-05, 'epoch': 1.33}
{'loss': 0.2719, 'grad_norm': 0.5672873258590698, 'learning_rate': 2.064896755162242e-05, 'epoch': 1.77}
{'loss': 0.1529, 'grad_norm': 0.07489565759897232, 'learning_rate': 1.3274336283185843e-05, 'epoch': 2.21}
{'loss': 0.1177, 'grad_norm': 0.10283011943101883, 'learning_rate': 5.899705014749263e-06, 'epoch': 2.65}
{'train_runtime': 26.3303, 'train_samples_per_second': 102.543, 'train_steps_per_second': 12.875, 'train_loss': 0.2851897952830897, 'epoch': 3.0}
{'loss': 0.6114, 'grad_norm': 5.258516788482666, 'learning_rate': 4.313725490196079e-05, 'epoch': 0.42}
{'loss': 0.4642, 'grad_norm': 9.075254440307617, 'learning_rate': 3.613445378151261e-05, 'epoch': 0.84}
{'loss': 0.4338, 'grad_norm': 14.61328125, 'learning_rate': 2.913165266106443e-05, 'epoch': 1.26}
{'loss': 0.3154, 'grad_norm': 24.730850219726562, 'learning_rate': 2.2128851540616248e-05, 'epoch': 1.68}
{'loss': 0.146, 'grad_norm': 0.35165587067604065, 'learning_rate': 1.5126050420168067e-05, 'epoch': 2.1}
{'loss': 0.1186, 'grad_norm': 0.08693073689937592, 'learning_rate': 8.123249299719889e-06, 'epoch': 2.52}
{'loss': 0.0573, 'grad_norm': 1.2456836700439453, 'learning_rate': 1.1204481792717088e-06, 'epoch': 2.94}
{'train_runtime': 27.5887, 'train_samples_per_second': 103.303, 'train_steps_per_second': 12.94, 'train_loss': 0.3007242501679422, 'epoch': 3.0}
{'loss': 0.5979, 'grad_norm': 21.905790328979492, 'learning_rate': 4.346666666666667e-05, 'epoch': 0.4}
{'loss': 0.5059, 'grad_norm': 5.846949100494385, 'learning_rate': 3.68e-05, 'epoch': 0.8}
{'loss': 0.3121, 'grad_norm': 0.5856296420097351, 'learning_rate': 3.0133333333333335e-05, 'epoch': 1.2}
{'loss': 0.2397, 'grad_norm': 87.51612854003906, 'learning_rate': 2.3466666666666667e-05, 'epoch': 1.6}
{'loss': 0.2191, 'grad_norm': 4.916631698608398, 'learning_rate': 1.6800000000000002e-05, 'epoch': 2.0}
{'loss': 0.0871, 'grad_norm': 0.12317139655351639, 'learning_rate': 1.0133333333333333e-05, 'epoch': 2.4}
{'loss': 0.0625, 'grad_norm': 0.5356449484825134, 'learning_rate': 3.466666666666667e-06, 'epoch': 2.8}
{'train_runtime': 30.1549, 'train_samples_per_second': 99.486, 'train_steps_per_second': 12.436, 'train_loss': 0.27984522247314453, 'epoch': 3.0}
{
    "model_name": "bert-base-cased",
    "assessment": "Data Efficiency",
    "task": "sst2",
    "details": {
        "100": {
            "accuracy": 0.658256880733945,
            "training_time_seconds": 5.328308343887329
        },
        "200": {
            "accuracy": 0.838302752293578,
            "training_time_seconds": 7.863286733627319
        },
        "300": {
            "accuracy": 0.8474770642201835,
            "training_time_seconds": 10.512310266494751
        },
        "400": {
            "accuracy": 0.8130733944954128,
            "training_time_seconds": 13.280779123306274
        },
        "500": {
            "accuracy": 0.8405963302752294,
            "training_time_seconds": 16.084529161453247
        },
        "600": {
            "accuracy": 0.8268348623853211,
            "training_time_seconds": 17.976330518722534
        },
        "700": {
            "accuracy": 0.8543577981651376,
            "training_time_seconds": 20.84261703491211
        },
        "examples_to_reach_target": 700,
        "800": {
            "accuracy": 0.8612385321100917,
            "training_time_seconds": 23.518030881881714
        },
        "900": {
            "accuracy": 0.8772935779816514,
            "training_time_seconds": 26.464698314666748
        },
        "950": {
            "accuracy": 0.8715596330275229,
            "training_time_seconds": 27.709779977798462
        },
        "1000": {
            "accuracy": 0.8692660550458715,
            "training_time_seconds": 30.277565479278564
        }
    }
}


================== Processing Model: roberta-base ==================

PHASE 0: Fine-Tuning...
Fine-tuned model for roberta-base already exists. Skipping fine-tuning.

PHASE 1: Static Size vs. Functional Utility...
Assessing Factual Recall for fine-tuned roberta-base...
{
    "model_path": "/root/test/E1/E1B/models/roberta-base-finetuned-local",
    "assessment": "Factual Recall from Local Data",
    "model_size_mb": 480.30399322509766,
    "factual_recall_score": 0.45
}

PHASE 2: Computational Cost & Adaptability...
NOTE: The following tasks use the base 'roberta-base' model for fair comparison.

Assessing Computational Cost for roberta-base...
{
    "model_name": "roberta-base",
    "assessment": "Computational Cost Analysis",
    "details": {
        "gflops_per_inference": 10.884513792,
        "avg_latency_ms_per_inference": 3.505692481994629,
        "sequence_length": 128
    }
}

Assessing Zero-Shot Reasoning for roberta-base...
{
    "model_name": "roberta-base",
    "assessment": "Zero-Shot NLI Reasoning",
    "task": "multi_nli",
    "accuracy": 0.3
}

Measuring Data Efficiency for roberta-base...
{'train_runtime': 5.5338, 'train_samples_per_second': 54.212, 'train_steps_per_second': 7.048, 'train_loss': 0.7044295775584686, 'epoch': 3.0}
{'loss': 0.6254, 'grad_norm': 33.670555114746094, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.0}
{'train_runtime': 7.9534, 'train_samples_per_second': 75.44, 'train_steps_per_second': 9.43, 'train_loss': 0.5339502843221029, 'epoch': 3.0}
{'loss': 0.605, 'grad_norm': 270.92071533203125, 'learning_rate': 2.850877192982456e-05, 'epoch': 1.32}
{'loss': 0.3886, 'grad_norm': 10.111260414123535, 'learning_rate': 6.578947368421053e-06, 'epoch': 2.63}
{'train_runtime': 10.6902, 'train_samples_per_second': 84.189, 'train_steps_per_second': 10.664, 'train_loss': 0.45498478412628174, 'epoch': 3.0}
{'loss': 0.6711, 'grad_norm': 24.240568161010742, 'learning_rate': 3.366666666666667e-05, 'epoch': 1.0}
{'loss': 0.3537, 'grad_norm': 0.367234468460083, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.0}
{'loss': 0.2335, 'grad_norm': 213.36557006835938, 'learning_rate': 3.3333333333333335e-07, 'epoch': 3.0}
{'train_runtime': 13.4847, 'train_samples_per_second': 88.99, 'train_steps_per_second': 11.124, 'train_loss': 0.4194214185078939, 'epoch': 3.0}
{'loss': 0.6405, 'grad_norm': 69.28462982177734, 'learning_rate': 3.7037037037037037e-05, 'epoch': 0.79}
{'loss': 0.5285, 'grad_norm': 1.3934826850891113, 'learning_rate': 2.380952380952381e-05, 'epoch': 1.59}
{'loss': 0.417, 'grad_norm': 5.289440155029297, 'learning_rate': 1.0582010582010582e-05, 'epoch': 2.38}
{'train_runtime': 16.1536, 'train_samples_per_second': 92.858, 'train_steps_per_second': 11.7, 'train_loss': 0.46352004500293226, 'epoch': 3.0}
{'loss': 0.6266, 'grad_norm': 13.459632873535156, 'learning_rate': 3.9111111111111115e-05, 'epoch': 0.67}
{'loss': 0.4178, 'grad_norm': 0.8212875723838806, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.33}
{'loss': 0.3828, 'grad_norm': 0.8995888233184814, 'learning_rate': 1.688888888888889e-05, 'epoch': 2.0}
{'loss': 0.1862, 'grad_norm': 89.71832275390625, 'learning_rate': 5.777777777777778e-06, 'epoch': 2.67}
{'train_runtime': 18.9766, 'train_samples_per_second': 94.854, 'train_steps_per_second': 11.857, 'train_loss': 0.3787845039367676, 'epoch': 3.0}
{'loss': 0.6867, 'grad_norm': 12.606856346130371, 'learning_rate': 4.071969696969698e-05, 'epoch': 0.57}
{'loss': 0.5403, 'grad_norm': 32.80760192871094, 'learning_rate': 3.125e-05, 'epoch': 1.14}
{'loss': 0.4235, 'grad_norm': 115.42504119873047, 'learning_rate': 2.178030303030303e-05, 'epoch': 1.7}
{'loss': 0.273, 'grad_norm': 0.19264431297779083, 'learning_rate': 1.2310606060606061e-05, 'epoch': 2.27}
{'loss': 0.2985, 'grad_norm': 0.10833510011434555, 'learning_rate': 2.840909090909091e-06, 'epoch': 2.84}
{'train_runtime': 21.7604, 'train_samples_per_second': 96.506, 'train_steps_per_second': 12.132, 'train_loss': 0.42687816872741235, 'epoch': 3.0}
{'loss': 0.6645, 'grad_norm': 8.233001708984375, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.5}
{'loss': 0.5557, 'grad_norm': 8.811594009399414, 'learning_rate': 3.35e-05, 'epoch': 1.0}
{'loss': 0.4237, 'grad_norm': 37.140499114990234, 'learning_rate': 2.5166666666666667e-05, 'epoch': 1.5}
{'loss': 0.3729, 'grad_norm': 15.419475555419922, 'learning_rate': 1.6833333333333334e-05, 'epoch': 2.0}
{'loss': 0.2191, 'grad_norm': 0.229928120970726, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.5}
{'loss': 0.1583, 'grad_norm': 7.88985013961792, 'learning_rate': 1.6666666666666668e-07, 'epoch': 3.0}
{'train_runtime': 24.4352, 'train_samples_per_second': 98.219, 'train_steps_per_second': 12.277, 'train_loss': 0.39900815645853677, 'epoch': 3.0}
{'loss': 0.6491, 'grad_norm': 1.9518760442733765, 'learning_rate': 4.2772861356932154e-05, 'epoch': 0.44}
{'loss': 0.5586, 'grad_norm': 41.70314407348633, 'learning_rate': 3.5398230088495574e-05, 'epoch': 0.88}
{'loss': 0.4397, 'grad_norm': 2.268860340118408, 'learning_rate': 2.8023598820059e-05, 'epoch': 1.33}
{'loss': 0.4148, 'grad_norm': 0.38657742738723755, 'learning_rate': 2.064896755162242e-05, 'epoch': 1.77}
{'loss': 0.3345, 'grad_norm': 1.2590203285217285, 'learning_rate': 1.3274336283185843e-05, 'epoch': 2.21}
{'loss': 0.2685, 'grad_norm': 0.4370820224285126, 'learning_rate': 5.899705014749263e-06, 'epoch': 2.65}
{'train_runtime': 27.1257, 'train_samples_per_second': 99.536, 'train_steps_per_second': 12.497, 'train_loss': 0.41869908729485705, 'epoch': 3.0}
{'loss': 0.6753, 'grad_norm': 4.393570423126221, 'learning_rate': 4.313725490196079e-05, 'epoch': 0.42}
{'loss': 0.4796, 'grad_norm': 2.070791721343994, 'learning_rate': 3.613445378151261e-05, 'epoch': 0.84}
{'loss': 0.4571, 'grad_norm': 19.76844024658203, 'learning_rate': 2.913165266106443e-05, 'epoch': 1.26}
{'loss': 0.3871, 'grad_norm': 29.05906105041504, 'learning_rate': 2.2128851540616248e-05, 'epoch': 1.68}
{'loss': 0.3129, 'grad_norm': 0.37069079279899597, 'learning_rate': 1.5126050420168067e-05, 'epoch': 2.1}
{'loss': 0.2097, 'grad_norm': 0.19757500290870667, 'learning_rate': 8.123249299719889e-06, 'epoch': 2.52}
{'loss': 0.2454, 'grad_norm': 16.361303329467773, 'learning_rate': 1.1204481792717088e-06, 'epoch': 2.94}
{'train_runtime': 28.6972, 'train_samples_per_second': 99.313, 'train_steps_per_second': 12.44, 'train_loss': 0.3957543079258681, 'epoch': 3.0}
{'loss': 0.6771, 'grad_norm': 15.854232788085938, 'learning_rate': 4.346666666666667e-05, 'epoch': 0.4}
{'loss': 0.4983, 'grad_norm': 108.2408447265625, 'learning_rate': 3.68e-05, 'epoch': 0.8}
{'loss': 0.4285, 'grad_norm': 110.9308090209961, 'learning_rate': 3.0133333333333335e-05, 'epoch': 1.2}
{'loss': 0.3396, 'grad_norm': 9.366286277770996, 'learning_rate': 2.3466666666666667e-05, 'epoch': 1.6}
{'loss': 0.3119, 'grad_norm': 0.12939199805259705, 'learning_rate': 1.6800000000000002e-05, 'epoch': 2.0}
{'loss': 0.1836, 'grad_norm': 11.672056198120117, 'learning_rate': 1.0133333333333333e-05, 'epoch': 2.4}
{'loss': 0.1888, 'grad_norm': 0.18465137481689453, 'learning_rate': 3.466666666666667e-06, 'epoch': 2.8}
{'train_runtime': 30.0526, 'train_samples_per_second': 99.825, 'train_steps_per_second': 12.478, 'train_loss': 0.3701553039550781, 'epoch': 3.0}
{
    "model_name": "roberta-base",
    "assessment": "Data Efficiency",
    "task": "sst2",
    "details": {
        "100": {
            "accuracy": 0.4908256880733945,
            "training_time_seconds": 5.651752948760986
        },
        "200": {
            "accuracy": 0.8520642201834863,
            "training_time_seconds": 8.077470779418945
        },
        "examples_to_reach_target": 200,
        "300": {
            "accuracy": 0.8772935779816514,
            "training_time_seconds": 10.815224170684814
        },
        "400": {
            "accuracy": 0.8887614678899083,
            "training_time_seconds": 13.609546422958374
        },
        "500": {
            "accuracy": 0.8876146788990825,
            "training_time_seconds": 16.278825521469116
        },
        "600": {
            "accuracy": 0.8807339449541285,
            "training_time_seconds": 19.101574897766113
        },
        "700": {
            "accuracy": 0.8853211009174312,
            "training_time_seconds": 21.885721921920776
        },
        "800": {
            "accuracy": 0.9048165137614679,
            "training_time_seconds": 24.56172204017639
        },
        "900": {
            "accuracy": 0.9036697247706422,
            "training_time_seconds": 27.251190662384033
        },
        "950": {
            "accuracy": 0.8990825688073395,
            "training_time_seconds": 28.822673559188843
        },
        "1000": {
            "accuracy": 0.9048165137614679,
            "training_time_seconds": 30.177570104599
        }
    }
}


================== Processing Model: distilroberta-base ==================

PHASE 0: Fine-Tuning...
Fine-tuning distilroberta-base on /root/test/E1/E1B/../../data/wikipedia.txt...

PHASE 1: Static Size vs. Functional Utility...
Assessing Factual Recall for fine-tuned distilroberta-base...
{
    "model_path": "/root/test/E1/E1B/models/distilroberta-base-finetuned-local",
    "assessment": "Factual Recall from Local Data",
    "model_size_mb": 318.06439876556396,
    "factual_recall_score": 0.45
}

PHASE 2: Computational Cost & Adaptability...
NOTE: The following tasks use the base 'distilroberta-base' model for fair comparison.

Assessing Computational Cost for distilroberta-base...
{
    "model_name": "distilroberta-base",
    "assessment": "Computational Cost Analysis",
    "details": {
        "gflops_per_inference": 5.442797568,
        "avg_latency_ms_per_inference": 1.9313907623291016,
        "sequence_length": 128
    }
}

Assessing Zero-Shot Reasoning for distilroberta-base...
{
    "model_name": "distilroberta-base",
    "assessment": "Zero-Shot NLI Reasoning",
    "task": "multi_nli",
    "accuracy": 0.3
}

Measuring Data Efficiency for distilroberta-base...
{'train_runtime': 3.5166, 'train_samples_per_second': 85.31, 'train_steps_per_second': 11.09, 'train_loss': 0.6120410332312951, 'epoch': 3.0}
{'loss': 0.5562, 'grad_norm': 66.72892761230469, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.0}
{'train_runtime': 4.5678, 'train_samples_per_second': 131.354, 'train_steps_per_second': 16.419, 'train_loss': 0.4748440424601237, 'epoch': 3.0}
{'loss': 0.6617, 'grad_norm': 55.34349822998047, 'learning_rate': 2.850877192982456e-05, 'epoch': 1.32}
{'loss': 0.3618, 'grad_norm': 33.05733108520508, 'learning_rate': 6.578947368421053e-06, 'epoch': 2.63}
{'train_runtime': 6.3946, 'train_samples_per_second': 140.743, 'train_steps_per_second': 17.827, 'train_loss': 0.4741157941650926, 'epoch': 3.0}
{'loss': 0.6605, 'grad_norm': 13.542853355407715, 'learning_rate': 3.366666666666667e-05, 'epoch': 1.0}
{'loss': 0.4607, 'grad_norm': 2.560426712036133, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.0}
{'loss': 0.1791, 'grad_norm': 0.2690141499042511, 'learning_rate': 3.3333333333333335e-07, 'epoch': 3.0}
{'train_runtime': 7.7968, 'train_samples_per_second': 153.91, 'train_steps_per_second': 19.239, 'train_loss': 0.4334435208638509, 'epoch': 3.0}
{'loss': 0.6602, 'grad_norm': 17.023839950561523, 'learning_rate': 3.7037037037037037e-05, 'epoch': 0.79}
{'loss': 0.4841, 'grad_norm': 3.1424734592437744, 'learning_rate': 2.380952380952381e-05, 'epoch': 1.59}
{'loss': 0.3301, 'grad_norm': 7.611917018890381, 'learning_rate': 1.0582010582010582e-05, 'epoch': 2.38}
{'train_runtime': 9.2123, 'train_samples_per_second': 162.826, 'train_steps_per_second': 20.516, 'train_loss': 0.4392142119231047, 'epoch': 3.0}
{'loss': 0.6438, 'grad_norm': 15.648016929626465, 'learning_rate': 3.9111111111111115e-05, 'epoch': 0.67}
{'loss': 0.4231, 'grad_norm': 25.197467803955078, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.33}
{'loss': 0.3021, 'grad_norm': 19.461734771728516, 'learning_rate': 1.688888888888889e-05, 'epoch': 2.0}
{'loss': 0.208, 'grad_norm': 144.73341369628906, 'learning_rate': 5.777777777777778e-06, 'epoch': 2.67}
{'train_runtime': 10.7219, 'train_samples_per_second': 167.88, 'train_steps_per_second': 20.985, 'train_loss': 0.37404817157321507, 'epoch': 3.0}
{'loss': 0.6058, 'grad_norm': 43.52187728881836, 'learning_rate': 4.071969696969698e-05, 'epoch': 0.57}
{'loss': 0.4581, 'grad_norm': 16.7891845703125, 'learning_rate': 3.125e-05, 'epoch': 1.14}
{'loss': 0.3775, 'grad_norm': 14.754532814025879, 'learning_rate': 2.178030303030303e-05, 'epoch': 1.7}
{'loss': 0.2875, 'grad_norm': 0.16647568345069885, 'learning_rate': 1.2310606060606061e-05, 'epoch': 2.27}
{'loss': 0.2351, 'grad_norm': 0.34209924936294556, 'learning_rate': 2.840909090909091e-06, 'epoch': 2.84}
{'train_runtime': 12.231, 'train_samples_per_second': 171.695, 'train_steps_per_second': 21.585, 'train_loss': 0.37774197125073633, 'epoch': 3.0}
{'loss': 0.5564, 'grad_norm': 5.508486270904541, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.5}
{'loss': 0.5386, 'grad_norm': 24.9608097076416, 'learning_rate': 3.35e-05, 'epoch': 1.0}
{'loss': 0.3299, 'grad_norm': 0.2673129439353943, 'learning_rate': 2.5166666666666667e-05, 'epoch': 1.5}
{'loss': 0.3072, 'grad_norm': 5.201798439025879, 'learning_rate': 1.6833333333333334e-05, 'epoch': 2.0}
{'loss': 0.1744, 'grad_norm': 10.155369758605957, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.5}
{'loss': 0.1066, 'grad_norm': 36.527828216552734, 'learning_rate': 1.6666666666666668e-07, 'epoch': 3.0}
{'train_runtime': 13.675, 'train_samples_per_second': 175.503, 'train_steps_per_second': 21.938, 'train_loss': 0.33552900950113934, 'epoch': 3.0}
{'loss': 0.6087, 'grad_norm': 57.68745040893555, 'learning_rate': 4.2772861356932154e-05, 'epoch': 0.44}
{'loss': 0.4961, 'grad_norm': 16.389869689941406, 'learning_rate': 3.5398230088495574e-05, 'epoch': 0.88}
{'loss': 0.4223, 'grad_norm': 2.412875175476074, 'learning_rate': 2.8023598820059e-05, 'epoch': 1.33}
{'loss': 0.3921, 'grad_norm': 0.49067819118499756, 'learning_rate': 2.064896755162242e-05, 'epoch': 1.77}
{'loss': 0.2168, 'grad_norm': 0.11907704919576645, 'learning_rate': 1.3274336283185843e-05, 'epoch': 2.21}
{'loss': 0.1243, 'grad_norm': 28.30126190185547, 'learning_rate': 5.899705014749263e-06, 'epoch': 2.65}
{'train_runtime': 15.2013, 'train_samples_per_second': 177.616, 'train_steps_per_second': 22.301, 'train_loss': 0.35181894836875893, 'epoch': 3.0}
{'loss': 0.5741, 'grad_norm': 41.77588653564453, 'learning_rate': 4.313725490196079e-05, 'epoch': 0.42}
{'loss': 0.4786, 'grad_norm': 81.87659454345703, 'learning_rate': 3.613445378151261e-05, 'epoch': 0.84}
{'loss': 0.4946, 'grad_norm': 98.84246826171875, 'learning_rate': 2.913165266106443e-05, 'epoch': 1.26}
{'loss': 0.3355, 'grad_norm': 13.511012077331543, 'learning_rate': 2.2128851540616248e-05, 'epoch': 1.68}
{'loss': 0.258, 'grad_norm': 0.2097674310207367, 'learning_rate': 1.5126050420168067e-05, 'epoch': 2.1}
{'loss': 0.1442, 'grad_norm': 0.09880679100751877, 'learning_rate': 8.123249299719889e-06, 'epoch': 2.52}
{'loss': 0.1809, 'grad_norm': 11.98706340789795, 'learning_rate': 1.1204481792717088e-06, 'epoch': 2.94}
{'train_runtime': 16.0009, 'train_samples_per_second': 178.115, 'train_steps_per_second': 22.311, 'train_loss': 0.35355324504756125, 'epoch': 3.0}
{'loss': 0.611, 'grad_norm': 35.68877029418945, 'learning_rate': 4.346666666666667e-05, 'epoch': 0.4}
{'loss': 0.5631, 'grad_norm': 17.872201919555664, 'learning_rate': 3.68e-05, 'epoch': 0.8}
{'loss': 0.3526, 'grad_norm': 0.8288248181343079, 'learning_rate': 3.0133333333333335e-05, 'epoch': 1.2}
{'loss': 0.3098, 'grad_norm': 13.139740943908691, 'learning_rate': 2.3466666666666667e-05, 'epoch': 1.6}
{'loss': 0.3071, 'grad_norm': 0.12323206663131714, 'learning_rate': 1.6800000000000002e-05, 'epoch': 2.0}
{'loss': 0.1442, 'grad_norm': 121.15657806396484, 'learning_rate': 1.0133333333333333e-05, 'epoch': 2.4}
{'loss': 0.1707, 'grad_norm': 0.9778027534484863, 'learning_rate': 3.466666666666667e-06, 'epoch': 2.8}
{'train_runtime': 16.6586, 'train_samples_per_second': 180.087, 'train_steps_per_second': 22.511, 'train_loss': 0.3400556805928548, 'epoch': 3.0}
{
    "model_name": "distilroberta-base",
    "assessment": "Data Efficiency",
    "task": "sst2",
    "details": {
        "100": {
            "accuracy": 0.7947247706422018,
            "training_time_seconds": 3.6323063373565674
        },
        "200": {
            "accuracy": 0.838302752293578,
            "training_time_seconds": 4.685688734054565
        },
        "300": {
            "accuracy": 0.8474770642201835,
            "training_time_seconds": 6.51348090171814
        },
        "400": {
            "accuracy": 0.8635321100917431,
            "training_time_seconds": 7.917806625366211
        },
        "examples_to_reach_target": 400,
        "500": {
            "accuracy": 0.8153669724770642,
            "training_time_seconds": 9.332058906555176
        },
        "600": {
            "accuracy": 0.8669724770642202,
            "training_time_seconds": 10.840785264968872
        },
        "700": {
            "accuracy": 0.8646788990825688,
            "training_time_seconds": 12.350047588348389
        },
        "800": {
            "accuracy": 0.8681192660550459,
            "training_time_seconds": 13.793821811676025
        },
        "900": {
            "accuracy": 0.8669724770642202,
            "training_time_seconds": 15.32068395614624
        },
        "950": {
            "accuracy": 0.8795871559633027,
            "training_time_seconds": 16.11969757080078
        },
        "1000": {
            "accuracy": 0.8922018348623854,
            "training_time_seconds": 16.777920246124268
        }
    }
}
==================================================================
Experiment 1B complete for all models. Results logged to /root/test/E1/E1B/E1B_local_results.log
